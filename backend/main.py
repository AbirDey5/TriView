# backend/main.py

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, Any
import datetime
import textwrap

# ------------------------------
# 1. Simple llm_call stub
# (replace with real Gemini code later, like in your Colab notebook)
# ------------------------------

def llm_call(system_prompt: str, user_prompt: str) -> str:
    return """STRENGTHS:
Demo mode: this feedback is from a stub LLM so you can test the frontend.
The draft has a clear idea and a helpful structure.

ISSUES:
Some sections could be more concise and focused.
Examples might make the explanation easier to follow.

SUGGESTIONS:
Tighten long sentences into shorter ones.
Add one concrete example to illustrate the main idea.

SEVERITY:
HIGH: Clarify the main message in the introduction.
MEDIUM: Remove redundant sentences.
LOW: Style and tone tweaks can come later.
"""

# ------------------------------
# 2. Multi-agent classes (very small JS-style version of your Python orchestrator)
# ------------------------------

class SimpleAgent:
    def __init__(self, name: str, role: str):
        self.name = name
        self.role = role

    def run(self, draft: str, domain: str, preferences: Dict[str, Any]):
        system_prompt = textwrap.dedent(f"""
        You are the {self.name} in a multi-agent writing feedback system.
        Role: {self.role}

        Return four sections exactly:

        STRENGTHS:
        ISSUES:
        SUGGESTIONS:
        SEVERITY:
        """)
        pref_text = "\n".join(f"- {k}: {v}" for k, v in preferences.items())
        user_prompt = textwrap.dedent(f"""
        Domain: {domain}

        Draft:
        <draft>
        {draft}
        </draft>

        Preferences:
        {pref_text}
        """)
        raw = llm_call(system_prompt, user_prompt)
        return self.parse_sections(raw)

    def parse_sections(self, text: str):
        sections = {"STRENGTHS": "", "ISSUES": "", "SUGGESTIONS": "", "SEVERITY": ""}
        current = None
        for line in text.splitlines():
            upper = line.strip().upper()
            if upper.startswith("STRENGTHS"):
                current = "STRENGTHS"; continue
            if upper.startswith("ISSUES"):
                current = "ISSUES"; continue
            if upper.startswith("SUGGESTIONS"):
                current = "SUGGESTIONS"; continue
            if upper.startswith("SEVERITY"):
                current = "SEVERITY"; continue
            if current:
                sections[current] += line + "\n"
        return {k.lower(): v.strip() for k, v in sections.items()}


peer_agent = SimpleAgent("Supportive Peer", "Be encouraging and focus on strengths and tone.")
editor_agent = SimpleAgent("Critical Editor", "Be tough on structure, logic, and conciseness.")
expert_agent = SimpleAgent("Domain Expert", "Check correctness, jargon, and audience fit.")


def build_action_plan(results):
    lines = ["Combined Action Plan", "--------------------"]
    for name, res in results.items():
        lines.append(f"From {name} - key issues:")
        lines.append(res["issues"] or "(none)")
        lines.append("")
        lines.append(f"From {name} - suggestions:")
        lines.append(res["suggestions"] or "(none)")
        lines.append("")
        lines.append(f"Severity notes ({name}):")
        lines.append(res["severity"] or "(none)")
        lines.append("")
    return "\n".join(lines)


def refine_draft(original: str, plan: str) -> str:
    # Stub refiner: in real version youâ€™d call Gemini here.
    return (
        "Refined draft (placeholder).\n\n"
        "In a real deployment, this would be generated by Gemini using the "
        "action plan below.\n\n"
        "----- ACTION PLAN -----\n"
        f"{plan}\n\n"
        "----- ORIGINAL DRAFT -----\n"
        f"{original}"
    )

# ------------------------------
# 3. FastAPI app
# ------------------------------

class FeedbackRequest(BaseModel):
    draft: str
    domain: str
    preferences: Dict[str, Any]


app = FastAPI()

# allow React dev server to call API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/api/feedback")
async def get_feedback(payload: FeedbackRequest):
    draft = payload.draft
    domain = payload.domain
    prefs = payload.preferences

    peer = peer_agent.run(draft, domain, prefs)
    editor = editor_agent.run(draft, domain, prefs)
    expert = expert_agent.run(draft, domain, prefs)

    results = {
        "Supportive Peer": peer,
        "Critical Editor": editor,
        "Domain Expert": expert,
    }

    plan = build_action_plan(results)
    refined = refine_draft(draft, plan)

    return {
        "peer_feedback": peer,
        "editor_feedback": editor,
        "expert_feedback": expert,
        "action_plan": plan,
        "refined_draft": refined,
        "timestamp": datetime.datetime.utcnow().isoformat(),
    }
